{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e5c7a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (1.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39451802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\hna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.22.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\HNA\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.2-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "     ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/14.8 MB 109.5 kB/s eta 0:02:16\n",
      "     --------------------------------------- 0.0/14.8 MB 122.9 kB/s eta 0:02:01\n",
      "     --------------------------------------- 0.0/14.8 MB 122.9 kB/s eta 0:02:01\n",
      "     --------------------------------------- 0.0/14.8 MB 122.9 kB/s eta 0:02:01\n",
      "     --------------------------------------- 0.1/14.8 MB 131.3 kB/s eta 0:01:53\n",
      "     --------------------------------------- 0.1/14.8 MB 140.6 kB/s eta 0:01:46\n",
      "     --------------------------------------- 0.1/14.8 MB 169.3 kB/s eta 0:01:28\n",
      "     --------------------------------------- 0.1/14.8 MB 198.8 kB/s eta 0:01:15\n",
      "     --------------------------------------- 0.1/14.8 MB 224.5 kB/s eta 0:01:06\n",
      "     --------------------------------------- 0.2/14.8 MB 235.4 kB/s eta 0:01:03\n",
      "      -------------------------------------- 0.2/14.8 MB 289.5 kB/s eta 0:00:51\n",
      "      -------------------------------------- 0.3/14.8 MB 342.3 kB/s eta 0:00:43\n",
      "      -------------------------------------- 0.3/14.8 MB 388.2 kB/s eta 0:00:38\n",
      "      -------------------------------------- 0.4/14.8 MB 441.4 kB/s eta 0:00:33\n",
      "     - ------------------------------------- 0.5/14.8 MB 524.5 kB/s eta 0:00:28\n",
      "     - ------------------------------------- 0.6/14.8 MB 621.6 kB/s eta 0:00:23\n",
      "     - ------------------------------------- 0.7/14.8 MB 730.7 kB/s eta 0:00:20\n",
      "     -- ------------------------------------ 0.9/14.8 MB 849.9 kB/s eta 0:00:17\n",
      "     -- ------------------------------------- 1.1/14.8 MB 1.0 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 1.3/14.8 MB 1.2 MB/s eta 0:00:12\n",
      "     ---- ----------------------------------- 1.6/14.8 MB 1.4 MB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 2.0/14.8 MB 1.7 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 2.1/14.8 MB 1.7 MB/s eta 0:00:08\n",
      "     ------- -------------------------------- 2.9/14.8 MB 2.3 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.7/14.8 MB 2.8 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 4.1/14.8 MB 3.0 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 4.6/14.8 MB 3.3 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 5.5/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.3/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 7.4/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 7.4/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 7.4/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 7.4/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 7.4/14.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 8.3/14.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 11.0/14.8 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 11.4/14.8 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 11.4/14.8 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 11.4/14.8 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 11.4/14.8 MB 7.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.8/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.8/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.8/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.8/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.8/14.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.8/14.8 MB 7.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.8/14.8 MB 7.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 14.8/14.8 MB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.1\n",
      "    Uninstalling numpy-1.22.1:\n",
      "      Successfully uninstalled numpy-1.22.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72642782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x204daa3dbb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a0c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"D:/data/celeb\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82a22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915b609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a24409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.02.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6a94f83",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (tuple), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mTraining Images\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m plt\u001b[39m.\u001b[39mimshow(np\u001b[39m.\u001b[39;49mtranspose(vutils\u001b[39m.\u001b[39;49mmake_grid(real_batch[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device)[:\u001b[39m64\u001b[39;49m], padding\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mcpu(),(\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m0\u001b[39;49m)))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mtranspose\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:660\u001b[0m, in \u001b[0;36mtranspose\u001b[1;34m(a, axes)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_transpose_dispatcher)\n\u001b[0;32m    602\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranspose\u001b[39m(a, axes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    603\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[39m    Returns an array with axes transposed.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[39m    For a 1-D array, this returns an unchanged view of the original array, as a\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[39m    transposed vector is simply the same vector.\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[39m    To convert a 1-D array into a 2-D column vector, an additional dimension\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[39m    must be added, e.g., ``np.atleast2d(a).T`` achieves this, as does\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[39m    ``a[:, np.newaxis]``.\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \u001b[39m    For a 2-D array, this is the standard matrix transpose.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[39m    For an n-D array, if axes are given, their order indicates how the\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[39m    axes are permuted (see Examples). If axes are not provided, then\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[39m    ``transpose(a).shape == a.shape[::-1]``.\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \n\u001b[0;32m    616\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[39m    a : array_like\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[39m        Input array.\u001b[39;00m\n\u001b[0;32m    620\u001b[0m \u001b[39m    axes : tuple or list of ints, optional\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[39m        If specified, it must be a tuple or list which contains a permutation\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39m        of [0,1,...,N-1] where N is the number of axes of `a`. The `i`'th axis\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[39m        of the returned array will correspond to the axis numbered ``axes[i]``\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[39m        of the input. If not specified, defaults to ``range(a.ndim)[::-1]``,\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[39m        which reverses the order of the axes.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[39m    -------\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[39m    p : ndarray\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[39m        `a` with its axes permuted. A view is returned whenever possible.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \n\u001b[0;32m    632\u001b[0m \u001b[39m    See Also\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[39m    --------\u001b[39;00m\n\u001b[0;32m    634\u001b[0m \u001b[39m    ndarray.transpose : Equivalent method.\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[39m    moveaxis : Move axes of an array to new positions.\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[39m    argsort : Return the indices that would sort an array.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \n\u001b[0;32m    638\u001b[0m \u001b[39m    Notes\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[39m    -----\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[39m    Use ``transpose(a, argsort(axes))`` to invert the transposition of tensors\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[39m    when using the `axes` keyword argument.\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \n\u001b[0;32m    643\u001b[0m \u001b[39m    Examples\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[39m    --------\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[39m    >>> a = np.array([[1, 2], [3, 4]])\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[39m    >>> a\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[39m    array([[1, 2],\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[39m           [3, 4]])\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[39m    >>> np.transpose(a)\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[39m    array([[1, 3],\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[39m           [2, 4]])\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \n\u001b[0;32m    653\u001b[0m \u001b[39m    >>> a = np.array([1, 2, 3, 4])\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[39m    >>> a\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[39m    array([1, 2, 3, 4])\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \u001b[39m    >>> np.transpose(a)\u001b[39;00m\n\u001b[0;32m    657\u001b[0m \u001b[39m    array([1, 2, 3, 4])\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \n\u001b[0;32m    659\u001b[0m \u001b[39m    >>> a = np.ones((1, 2, 3))\u001b[39;00m\n\u001b[1;32m--> 660\u001b[0m \u001b[39m    >>> np.transpose(a, (1, 0, 2)).shape\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[39m    (2, 1, 3)\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \n\u001b[0;32m    663\u001b[0m \u001b[39m    >>> a = np.ones((2, 3, 4, 5))\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[39m    >>> np.transpose(a).shape\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[39m    (5, 4, 3, 2)\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \n\u001b[0;32m    667\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    668\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39mtranspose\u001b[39m\u001b[39m'\u001b[39m, axes)\n",
      "File \u001b[1;32mc:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:66\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "File \u001b[1;32mc:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e9650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed26a436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac622d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21249ff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 231, in __getitem__\n    sample = self.transform(sample)\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 137, in __call__\n    return F.to_tensor(pic)\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py\", line 166, in to_tensor\n    img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))\nRuntimeError: Numpy is not available\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m# For each epoch\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     12\u001b[0m     \u001b[39m# For each batch in the dataloader\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader, \u001b[39m0\u001b[39m):\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m         \u001b[39m############################\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         \u001b[39m# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         \u001b[39m###########################\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         \u001b[39m## Train with all-real batch\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         netD\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     20\u001b[0m         \u001b[39m# Format batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[1;32mc:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[0;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 231, in __getitem__\n    sample = self.transform(sample)\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 137, in __call__\n    return F.to_tensor(pic)\n  File \"c:\\Users\\HNA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py\", line 166, in to_tensor\n    img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))\nRuntimeError: Numpy is not available\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe61be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db59ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f74ece",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG, \"D:/data/model/netG_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netD, \"D:/data/model/netD_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de2248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
